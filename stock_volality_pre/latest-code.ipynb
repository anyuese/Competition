{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33716a9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-07T00:25:05.107752Z",
     "iopub.status.busy": "2021-09-07T00:25:05.106498Z",
     "iopub.status.idle": "2021-09-07T00:25:06.310981Z",
     "shell.execute_reply": "2021-09-07T00:25:06.310197Z",
     "shell.execute_reply.started": "2021-08-26T03:55:40.361982Z"
    },
    "papermill": {
     "duration": 1.231678,
     "end_time": "2021-09-07T00:25:06.311161",
     "exception": false,
     "start_time": "2021-09-07T00:25:05.079483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "\n",
    "path_submissions = '/'\n",
    "\n",
    "target_name = 'target'\n",
    "scores_folds = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23545b7",
   "metadata": {
    "papermill": {
     "duration": 0.021999,
     "end_time": "2021-09-07T00:25:06.355431",
     "exception": false,
     "start_time": "2021-09-07T00:25:06.333432",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# # **You can consider giving an upvote, if you find it beneficial.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b672c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T00:25:06.452169Z",
     "iopub.status.busy": "2021-09-07T00:25:06.422126Z",
     "iopub.status.idle": "2021-09-07T00:25:06.476572Z",
     "shell.execute_reply": "2021-09-07T00:25:06.477403Z",
     "shell.execute_reply.started": "2021-08-26T03:55:41.744001Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.101222,
     "end_time": "2021-09-07T00:25:06.477656",
     "exception": false,
     "start_time": "2021-09-07T00:25:06.376434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "# Remember that logb(x / y) = logb(x) - logb(y)\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to count unique elements of a series\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return train, test\n",
    "\n",
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.std],\n",
    "        'wap2': [np.sum, np.std],\n",
    "        'wap3': [np.sum, np.std],\n",
    "        'wap4': [np.sum, np.std],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'wap_balance': [np.sum, np.max],\n",
    "        'price_spread':[np.sum, np.max],\n",
    "        'price_spread2':[np.sum, np.max],\n",
    "        'bid_spread':[np.sum, np.max],\n",
    "        'ask_spread':[np.sum, np.max],\n",
    "        'total_volume':[np.sum, np.max],\n",
    "        'volume_imbalance':[np.sum, np.max],\n",
    "        \"bid_ask_spread\":[np.sum,  np.max],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.max, np.min],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max,np.min],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        # new\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "        # vol vars\n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7190bc8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T00:25:06.548188Z",
     "iopub.status.busy": "2021-09-07T00:25:06.547289Z",
     "iopub.status.idle": "2021-09-07T01:06:15.628875Z",
     "shell.execute_reply": "2021-09-07T01:06:15.628289Z",
     "shell.execute_reply.started": "2021-08-26T03:55:41.826159Z"
    },
    "papermill": {
     "duration": 2469.115819,
     "end_time": "2021-09-07T01:06:15.629027",
     "exception": false,
     "start_time": "2021-09-07T00:25:06.513208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed: 41.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "train, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "train = get_time_stock(train)\n",
    "test = get_time_stock(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51ca599b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:06:15.688034Z",
     "iopub.status.busy": "2021-09-07T01:06:15.687381Z",
     "iopub.status.idle": "2021-09-07T01:06:15.704629Z",
     "shell.execute_reply": "2021-09-07T01:06:15.705073Z",
     "shell.execute_reply.started": "2021-08-26T04:39:00.80923Z"
    },
    "papermill": {
     "duration": 0.052186,
     "end_time": "2021-09-07T01:06:15.705259",
     "exception": false,
     "start_time": "2021-09-07T01:06:15.653073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace by order sum (tau)\n",
    "train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n",
    "#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\n",
    "train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n",
    "#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\n",
    "train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "795a619c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:06:15.760941Z",
     "iopub.status.busy": "2021-09-07T01:06:15.759691Z",
     "iopub.status.idle": "2021-09-07T01:06:15.778662Z",
     "shell.execute_reply": "2021-09-07T01:06:15.779168Z",
     "shell.execute_reply.started": "2021-08-26T04:39:00.839322Z"
    },
    "papermill": {
     "duration": 0.050317,
     "end_time": "2021-09-07T01:06:15.779352",
     "exception": false,
     "start_time": "2021-09-07T01:06:15.729035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "# delta tau\n",
    "train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "863911e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:06:15.830042Z",
     "iopub.status.busy": "2021-09-07T01:06:15.829388Z",
     "iopub.status.idle": "2021-09-07T01:06:15.837028Z",
     "shell.execute_reply": "2021-09-07T01:06:15.837541Z",
     "shell.execute_reply.started": "2021-08-26T04:39:00.878555Z"
    },
    "papermill": {
     "duration": 0.034907,
     "end_time": "2021-09-07T01:06:15.837688",
     "exception": false,
     "start_time": "2021-09-07T01:06:15.802781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colNames = [col for col in list(train.columns)\n",
    "            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\n",
    "len(colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b619360",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:06:15.888336Z",
     "iopub.status.busy": "2021-09-07T01:06:15.887729Z",
     "iopub.status.idle": "2021-09-07T01:06:18.030003Z",
     "shell.execute_reply": "2021-09-07T01:06:18.030837Z",
     "shell.execute_reply.started": "2021-08-26T04:39:00.890036Z"
    },
    "papermill": {
     "duration": 2.169447,
     "end_time": "2021-09-07T01:06:18.031085",
     "exception": false,
     "start_time": "2021-09-07T01:06:15.861638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# making agg features\n",
    "\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train.loc[train['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "729305cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:06:18.092505Z",
     "iopub.status.busy": "2021-09-07T01:06:18.090933Z",
     "iopub.status.idle": "2021-09-07T01:06:18.265084Z",
     "shell.execute_reply": "2021-09-07T01:06:18.264336Z",
     "shell.execute_reply.started": "2021-08-26T04:39:03.227624Z"
    },
    "papermill": {
     "duration": 0.208695,
     "end_time": "2021-09-07T01:06:18.265270",
     "exception": false,
     "start_time": "2021-09-07T01:06:18.056575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b12ebda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:06:18.329010Z",
     "iopub.status.busy": "2021-09-07T01:06:18.328389Z",
     "iopub.status.idle": "2021-09-07T01:06:26.446275Z",
     "shell.execute_reply": "2021-09-07T01:06:26.445608Z",
     "shell.execute_reply.started": "2021-08-26T04:39:03.433782Z"
    },
    "papermill": {
     "duration": 8.154324,
     "end_time": "2021-09-07T01:06:26.446419",
     "exception": false,
     "start_time": "2021-09-07T01:06:18.292095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] \n",
    "train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22cf42ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:06:26.504009Z",
     "iopub.status.busy": "2021-09-07T01:06:26.503196Z",
     "iopub.status.idle": "2021-09-07T01:09:15.830914Z",
     "shell.execute_reply": "2021-09-07T01:09:15.830347Z"
    },
    "papermill": {
     "duration": 169.357865,
     "end_time": "2021-09-07T01:09:15.831082",
     "exception": false,
     "start_time": "2021-09-07T01:06:26.473217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.to_csv('df_train.csv')\n",
    "test.to_csv('df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "222886bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:09:16.014171Z",
     "iopub.status.busy": "2021-09-07T01:09:16.013323Z",
     "iopub.status.idle": "2021-09-07T01:09:16.016778Z",
     "shell.execute_reply": "2021-09-07T01:09:16.017283Z",
     "shell.execute_reply.started": "2021-08-26T04:39:12.055844Z"
    },
    "papermill": {
     "duration": 0.155946,
     "end_time": "2021-09-07T01:09:16.017464",
     "exception": false,
     "start_time": "2021-09-07T01:09:15.861518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del mat1,mat2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233cb08",
   "metadata": {
    "papermill": {
     "duration": 0.027192,
     "end_time": "2021-09-07T01:09:16.071702",
     "exception": false,
     "start_time": "2021-09-07T01:09:16.044510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25a6b750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:09:16.130075Z",
     "iopub.status.busy": "2021-09-07T01:09:16.129063Z",
     "iopub.status.idle": "2021-09-07T01:21:12.405253Z",
     "shell.execute_reply": "2021-09-07T01:21:12.405705Z",
     "shell.execute_reply.started": "2021-08-26T04:39:12.232241Z"
    },
    "papermill": {
     "duration": 716.307353,
     "end_time": "2021-09-07T01:21:12.405871",
     "exception": false,
     "start_time": "2021-09-07T01:09:16.098518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428698\ttraining's RMSPE: 0.198264\tvalid_1's rmse: 0.00043934\tvalid_1's RMSPE: 0.203917\n",
      "[500]\ttraining's rmse: 0.000406922\ttraining's RMSPE: 0.188193\tvalid_1's rmse: 0.000425343\tvalid_1's RMSPE: 0.19742\n",
      "[750]\ttraining's rmse: 0.000392958\ttraining's RMSPE: 0.181735\tvalid_1's rmse: 0.000416835\tvalid_1's RMSPE: 0.193471\n",
      "[1000]\ttraining's rmse: 0.000383231\ttraining's RMSPE: 0.177236\tvalid_1's rmse: 0.000412414\tvalid_1's RMSPE: 0.191419\n",
      "[1250]\ttraining's rmse: 0.00037525\ttraining's RMSPE: 0.173545\tvalid_1's rmse: 0.000409508\tvalid_1's RMSPE: 0.190071\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1300]\ttraining's rmse: 0.000373807\ttraining's RMSPE: 0.172878\tvalid_1's rmse: 0.000408929\tvalid_1's RMSPE: 0.189802\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428653\ttraining's RMSPE: 0.19859\tvalid_1's rmse: 0.000440463\tvalid_1's RMSPE: 0.203012\n",
      "[500]\ttraining's rmse: 0.000406379\ttraining's RMSPE: 0.188271\tvalid_1's rmse: 0.000425018\tvalid_1's RMSPE: 0.195894\n",
      "[750]\ttraining's rmse: 0.000392668\ttraining's RMSPE: 0.181918\tvalid_1's rmse: 0.000417108\tvalid_1's RMSPE: 0.192248\n",
      "[1000]\ttraining's rmse: 0.000382831\ttraining's RMSPE: 0.177361\tvalid_1's rmse: 0.000412924\tvalid_1's RMSPE: 0.19032\n",
      "[1250]\ttraining's rmse: 0.00037469\ttraining's RMSPE: 0.173589\tvalid_1's rmse: 0.000409702\tvalid_1's RMSPE: 0.188835\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1300]\ttraining's rmse: 0.000373263\ttraining's RMSPE: 0.172928\tvalid_1's rmse: 0.000409234\tvalid_1's RMSPE: 0.188619\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.00042857\ttraining's RMSPE: 0.198198\tvalid_1's rmse: 0.000465999\tvalid_1's RMSPE: 0.21632\n",
      "[500]\ttraining's rmse: 0.000406477\ttraining's RMSPE: 0.187981\tvalid_1's rmse: 0.000453135\tvalid_1's RMSPE: 0.210349\n",
      "[750]\ttraining's rmse: 0.000392795\ttraining's RMSPE: 0.181653\tvalid_1's rmse: 0.000445963\tvalid_1's RMSPE: 0.207019\n",
      "[1000]\ttraining's rmse: 0.000383092\ttraining's RMSPE: 0.177166\tvalid_1's rmse: 0.000441162\tvalid_1's RMSPE: 0.204791\n",
      "[1250]\ttraining's rmse: 0.000374875\ttraining's RMSPE: 0.173366\tvalid_1's rmse: 0.000438083\tvalid_1's RMSPE: 0.203361\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1300]\ttraining's rmse: 0.000373456\ttraining's RMSPE: 0.17271\tvalid_1's rmse: 0.00043764\tvalid_1's RMSPE: 0.203156\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.00042854\ttraining's RMSPE: 0.198538\tvalid_1's rmse: 0.0004457\tvalid_1's RMSPE: 0.205425\n",
      "[500]\ttraining's rmse: 0.000406238\ttraining's RMSPE: 0.188205\tvalid_1's rmse: 0.00043002\tvalid_1's RMSPE: 0.198198\n",
      "[750]\ttraining's rmse: 0.000392756\ttraining's RMSPE: 0.18196\tvalid_1's rmse: 0.000422586\tvalid_1's RMSPE: 0.194771\n",
      "[1000]\ttraining's rmse: 0.000382971\ttraining's RMSPE: 0.177426\tvalid_1's rmse: 0.000418005\tvalid_1's RMSPE: 0.19266\n",
      "[1250]\ttraining's rmse: 0.000374918\ttraining's RMSPE: 0.173695\tvalid_1's rmse: 0.00041472\tvalid_1's RMSPE: 0.191146\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1300]\ttraining's rmse: 0.000373443\ttraining's RMSPE: 0.173012\tvalid_1's rmse: 0.000414392\tvalid_1's RMSPE: 0.190995\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000429751\ttraining's RMSPE: 0.198779\tvalid_1's rmse: 0.000442733\tvalid_1's RMSPE: 0.205379\n",
      "[500]\ttraining's rmse: 0.000407766\ttraining's RMSPE: 0.188609\tvalid_1's rmse: 0.000428896\tvalid_1's RMSPE: 0.19896\n",
      "[750]\ttraining's rmse: 0.000394267\ttraining's RMSPE: 0.182366\tvalid_1's rmse: 0.000421757\tvalid_1's RMSPE: 0.195648\n",
      "[1000]\ttraining's rmse: 0.000383985\ttraining's RMSPE: 0.17761\tvalid_1's rmse: 0.000417711\tvalid_1's RMSPE: 0.193771\n",
      "[1250]\ttraining's rmse: 0.000375959\ttraining's RMSPE: 0.173897\tvalid_1's rmse: 0.000414695\tvalid_1's RMSPE: 0.192372\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1300]\ttraining's rmse: 0.000374547\ttraining's RMSPE: 0.173244\tvalid_1's rmse: 0.000414252\tvalid_1's RMSPE: 0.192167\n",
      "Our out of folds RMSPE is 0.19301876111805275\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "seed0=2021\n",
    "params0 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'max_bin':100,\n",
    "    'min_data_in_leaf':500,\n",
    "    'learning_rate': 0.05, #0.05\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.5,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "    'categorical_column':[0],\n",
    "    'seed':seed0,\n",
    "    'feature_fraction_seed': seed0,\n",
    "    'bagging_seed': seed0,\n",
    "    'drop_seed': seed0,\n",
    "    'data_random_seed': seed0,\n",
    "    'n_jobs':-1,\n",
    "    'verbose': -1}\n",
    "seed1=42\n",
    "params1 = {\n",
    "        'learning_rate': 0.1,  #0.1      \n",
    "        'lambda_l1': 2,\n",
    "        'lambda_l2': 7,\n",
    "        'num_leaves': 800,\n",
    "        'min_sum_hessian_in_leaf': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'feature_fraction_bynode': 0.8,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 42,\n",
    "        'min_data_in_leaf': 700,\n",
    "        'max_depth': 4,\n",
    "        'categorical_column':[0],\n",
    "        'seed': seed1,\n",
    "        'feature_fraction_seed': seed1,\n",
    "        'bagging_seed': seed1,\n",
    "        'drop_seed': seed1,\n",
    "        'data_random_seed': seed1,\n",
    "        'objective': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs':-1,\n",
    "    }\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params):\n",
    "    # Hyperparammeters (just basic)\n",
    "    \n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "    # Create out of folds array\n",
    "    y = train['target']\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n",
    "        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n",
    "        model = lgb.train(params = params,\n",
    "                          num_boost_round=1300,\n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          verbose_eval = 250,\n",
    "                          early_stopping_rounds=50, #50\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(test[features]) / 5\n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    lgb.plot_importance(model,max_num_features=20)\n",
    "    # Return test predictions\n",
    "    return test_predictions\n",
    "# Traing and evaluate\n",
    "predictions_lgb_1= train_and_evaluate_lgb(train, test,params0)\n",
    "# predictions_lgb_2= train_and_evaluate_lgb(train, test,params1)\n",
    "# test['target'] = predictions_lgb_1\n",
    "# test[['row_id', 'target']].to_csv('submission.csv',index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2e5429d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:21:12.510414Z",
     "iopub.status.busy": "2021-09-07T01:21:12.509429Z",
     "iopub.status.idle": "2021-09-07T01:21:12.514843Z",
     "shell.execute_reply": "2021-09-07T01:21:12.514180Z",
     "shell.execute_reply.started": "2021-08-26T04:48:53.093706Z"
    },
    "papermill": {
     "duration": 0.062222,
     "end_time": "2021-09-07T01:21:12.514990",
     "exception": false,
     "start_time": "2021-09-07T01:21:12.452768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b2510dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:21:12.611588Z",
     "iopub.status.busy": "2021-09-07T01:21:12.610642Z",
     "iopub.status.idle": "2021-09-07T01:21:18.336543Z",
     "shell.execute_reply": "2021-09-07T01:21:18.336002Z",
     "shell.execute_reply.started": "2021-08-26T04:48:53.10191Z"
    },
    "papermill": {
     "duration": 5.771314,
     "end_time": "2021-09-07T01:21:18.336706",
     "exception": false,
     "start_time": "2021-09-07T01:21:12.565392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
    "    \n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a880c385",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:21:18.427008Z",
     "iopub.status.busy": "2021-09-07T01:21:18.421779Z",
     "iopub.status.idle": "2021-09-07T01:21:29.192181Z",
     "shell.execute_reply": "2021-09-07T01:21:29.191514Z",
     "shell.execute_reply.started": "2021-08-26T05:02:23.375706Z"
    },
    "papermill": {
     "duration": 10.814688,
     "end_time": "2021-09-07T01:21:29.192336",
     "exception": false,
     "start_time": "2021-09-07T01:21:18.377648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kfold based on the knn++ algorithm\n",
    "\n",
    "out_train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "#out_train[out_train.isna().any(axis=1)]\n",
    "out_train = out_train.fillna(out_train.mean())\n",
    "out_train.head()\n",
    "\n",
    "# code to add the just the read data after first execution\n",
    "\n",
    "# data separation based on knn ++\n",
    "nfolds = 5 #5 # number of folds\n",
    "index = []\n",
    "totDist = []\n",
    "values = []\n",
    "# generates a matriz with the values of \n",
    "mat = out_train.values\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "mat = scaler.fit_transform(mat)\n",
    "\n",
    "nind = int(mat.shape[0]/nfolds) # number of individuals\n",
    "\n",
    "# adds index in the last column\n",
    "mat = np.c_[mat,np.arange(mat.shape[0])]\n",
    "\n",
    "\n",
    "lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n",
    "\n",
    "lineNumber = np.sort(lineNumber)[::-1]\n",
    "\n",
    "for n in range(nfolds):\n",
    "    totDist.append(np.zeros(mat.shape[0]-nfolds))\n",
    "\n",
    "# saves index\n",
    "for n in range(nfolds):\n",
    "    \n",
    "    values.append([lineNumber[n]])    \n",
    "\n",
    "\n",
    "s=[]\n",
    "for n in range(nfolds):\n",
    "    s.append(mat[lineNumber[n],:])\n",
    "    \n",
    "    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n",
    "\n",
    "for n in range(nind-1):    \n",
    "\n",
    "    luck = np.random.uniform(0,1,nfolds)\n",
    "    \n",
    "    for cycle in range(nfolds):\n",
    "         # saves the values of index           \n",
    "\n",
    "        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n",
    "\n",
    "        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n",
    "        totDist[cycle] += sumDist        \n",
    "                \n",
    "        # probabilities\n",
    "        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n",
    "        j = 0\n",
    "        kn = 0\n",
    "        for val in f:\n",
    "            j += val        \n",
    "            if (j > luck[cycle]): # the column was selected\n",
    "                break\n",
    "            kn +=1\n",
    "        lineNumber[cycle] = kn\n",
    "        \n",
    "        # delete line of the value added    \n",
    "        for n_iter in range(nfolds):\n",
    "            \n",
    "            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n",
    "            j= 0\n",
    "        \n",
    "        s[cycle] = mat[lineNumber[cycle],:]\n",
    "        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n",
    "        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n",
    "\n",
    "\n",
    "for n_mod in range(nfolds):\n",
    "    values[n_mod] = out_train.index[values[n_mod]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "815f6ec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:21:29.283126Z",
     "iopub.status.busy": "2021-09-07T01:21:29.282464Z",
     "iopub.status.idle": "2021-09-07T01:21:57.034829Z",
     "shell.execute_reply": "2021-09-07T01:21:57.034213Z",
     "shell.execute_reply.started": "2021-08-26T05:04:30.178233Z"
    },
    "papermill": {
     "duration": 27.800788,
     "end_time": "2021-09-07T01:21:57.034972",
     "exception": false,
     "start_time": "2021-09-07T01:21:29.234184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#colNames.remove('row_id')\n",
    "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "qt_train = []\n",
    "train_nn=train[colNames].copy()\n",
    "test_nn=test[colNames].copy()\n",
    "for col in colNames:\n",
    "    #print(col)\n",
    "    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n",
    "    train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
    "    test_nn[col] = qt.transform(test_nn[[col]])    \n",
    "    qt_train.append(qt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d24d0fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:21:57.126345Z",
     "iopub.status.busy": "2021-09-07T01:21:57.125656Z",
     "iopub.status.idle": "2021-09-07T01:21:57.134550Z",
     "shell.execute_reply": "2021-09-07T01:21:57.134050Z",
     "shell.execute_reply.started": "2021-08-26T05:04:59.298556Z"
    },
    "papermill": {
     "duration": 0.05847,
     "end_time": "2021-09-07T01:21:57.134700",
     "exception": false,
     "start_time": "2021-09-07T01:21:57.076230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\n",
    "test_nn[['stock_id','time_id']]=test[['stock_id','time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f05d24a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:21:57.227868Z",
     "iopub.status.busy": "2021-09-07T01:21:57.227204Z",
     "iopub.status.idle": "2021-09-07T01:21:58.861287Z",
     "shell.execute_reply": "2021-09-07T01:21:58.860759Z",
     "shell.execute_reply.started": "2021-08-26T05:04:59.318065Z"
    },
    "papermill": {
     "duration": 1.685818,
     "end_time": "2021-09-07T01:21:58.861422",
     "exception": false,
     "start_time": "2021-09-07T01:21:57.175604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "# making agg features\n",
    "from sklearn.cluster import KMeans\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57229eb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:21:58.955224Z",
     "iopub.status.busy": "2021-09-07T01:21:58.954534Z",
     "iopub.status.idle": "2021-09-07T01:21:58.957080Z",
     "shell.execute_reply": "2021-09-07T01:21:58.957622Z",
     "shell.execute_reply.started": "2021-08-26T05:05:08.044692Z"
    },
    "papermill": {
     "duration": 0.052937,
     "end_time": "2021-09-07T01:21:58.957780",
     "exception": false,
     "start_time": "2021-09-07T01:21:58.904843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d06d6a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:21:59.048578Z",
     "iopub.status.busy": "2021-09-07T01:21:59.047831Z",
     "iopub.status.idle": "2021-09-07T01:21:59.226657Z",
     "shell.execute_reply": "2021-09-07T01:21:59.227202Z",
     "shell.execute_reply.started": "2021-08-26T05:05:10.83776Z"
    },
    "papermill": {
     "duration": 0.227026,
     "end_time": "2021-09-07T01:21:59.227373",
     "exception": false,
     "start_time": "2021-09-07T01:21:59.000347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b13fa280",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:21:59.322009Z",
     "iopub.status.busy": "2021-09-07T01:21:59.320636Z",
     "iopub.status.idle": "2021-09-07T01:22:04.748947Z",
     "shell.execute_reply": "2021-09-07T01:22:04.748312Z",
     "shell.execute_reply.started": "2021-08-26T05:05:13.957209Z"
    },
    "papermill": {
     "duration": 5.47877,
     "end_time": "2021-09-07T01:22:04.749088",
     "exception": false,
     "start_time": "2021-09-07T01:21:59.270318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "train_nn = pd.merge(train_nn,mat1[nnn],how='left',on='time_id')\n",
    "test_nn = pd.merge(test_nn,mat2[nnn],how='left',on='time_id')\n",
    "del mat1,mat2\n",
    "del train,test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "768e9704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:22:04.847635Z",
     "iopub.status.busy": "2021-09-07T01:22:04.846952Z",
     "iopub.status.idle": "2021-09-07T01:22:04.866780Z",
     "shell.execute_reply": "2021-09-07T01:22:04.867372Z",
     "shell.execute_reply.started": "2021-08-26T05:07:40.604977Z"
    },
    "papermill": {
     "duration": 0.075035,
     "end_time": "2021-09-07T01:22:04.867544",
     "exception": false,
     "start_time": "2021-09-07T01:22:04.792509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\n",
    "from keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "hidden_units = (128,64,32)\n",
    "stock_embedding_size = 24\n",
    "\n",
    "cat_data = train_nn['stock_id']\n",
    "\n",
    "def base_model():\n",
    "    \n",
    "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(244,), name='num_data')\n",
    "\n",
    "\n",
    "    #embedding, flatenning and concatenating\n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "        \n",
    "\n",
    "    #out = keras.layers.Concatenate()([out, num_input])\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "580ac5bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:22:04.955860Z",
     "iopub.status.busy": "2021-09-07T01:22:04.955233Z",
     "iopub.status.idle": "2021-09-07T01:36:05.766938Z",
     "shell.execute_reply": "2021-09-07T01:36:05.711674Z",
     "shell.execute_reply.started": "2021-08-26T05:07:43.746892Z"
    },
    "papermill": {
     "duration": 840.85682,
     "end_time": "2021-09-07T01:36:05.767158",
     "exception": false,
     "start_time": "2021-09-07T01:22:04.910338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 4s 17ms/step - loss: 23.6953 - val_loss: 1.7192\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.9857 - val_loss: 0.5698\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6524 - val_loss: 0.5460\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6289 - val_loss: 0.7143\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.5223 - val_loss: 0.7325\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.9121 - val_loss: 0.7280\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6053 - val_loss: 0.5918\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.5474 - val_loss: 0.5482\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5969 - val_loss: 0.6247\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6996 - val_loss: 0.3952\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4268 - val_loss: 0.5293\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4212 - val_loss: 0.4298\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4442 - val_loss: 0.3448\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.3713 - val_loss: 0.2358\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2307 - val_loss: 0.2692\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2302 - val_loss: 0.2331\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2391 - val_loss: 0.2683\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.4012 - val_loss: 0.2457\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2456 - val_loss: 0.2255\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2290 - val_loss: 0.2152\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2257 - val_loss: 0.2235\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2268 - val_loss: 0.2654\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2242 - val_loss: 0.2178\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2210 - val_loss: 0.2153\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2248 - val_loss: 0.2362\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2332 - val_loss: 0.2676\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2474 - val_loss: 0.2359\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2137 - val_loss: 0.2093\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2103 - val_loss: 0.2099\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2095 - val_loss: 0.2107\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2099 - val_loss: 0.2101\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2094 - val_loss: 0.2109\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2094 - val_loss: 0.2091\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2088 - val_loss: 0.2139\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2088 - val_loss: 0.2122\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2095 - val_loss: 0.2104\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2084 - val_loss: 0.2096\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2098 - val_loss: 0.2090\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2088 - val_loss: 0.2094\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2097 - val_loss: 0.2121\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2065 - val_loss: 0.2088\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2059 - val_loss: 0.2087\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2064 - val_loss: 0.2091\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2069 - val_loss: 0.2090\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2061 - val_loss: 0.2082\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2069 - val_loss: 0.2081\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2061 - val_loss: 0.2093\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2051 - val_loss: 0.2100\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2059 - val_loss: 0.2087\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2057 - val_loss: 0.2101\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2069 - val_loss: 0.2092\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2063 - val_loss: 0.2084\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2057 - val_loss: 0.2102\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2054 - val_loss: 0.2081\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2048 - val_loss: 0.2084\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2084\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2040 - val_loss: 0.2083\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2046 - val_loss: 0.2084\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2035 - val_loss: 0.2088\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2047 - val_loss: 0.2087\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2039 - val_loss: 0.2082\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2040 - val_loss: 0.2081\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2081\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2036 - val_loss: 0.2081\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2043 - val_loss: 0.2082\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2081\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2036 - val_loss: 0.2081\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2050 - val_loss: 0.2081\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2033 - val_loss: 0.2081\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2037 - val_loss: 0.2082\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2037 - val_loss: 0.2081\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2048 - val_loss: 0.2081\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2033 - val_loss: 0.2083\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2034 - val_loss: 0.2082\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2044 - val_loss: 0.2082\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2039 - val_loss: 0.2081\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2042 - val_loss: 0.2081\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2041 - val_loss: 0.2081\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2047 - val_loss: 0.2081\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2037 - val_loss: 0.2081\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2081\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2035 - val_loss: 0.2081\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2037 - val_loss: 0.2081\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2039 - val_loss: 0.2081\n",
      "Fold 1 NN: 0.20807\n",
      "CV 2/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 14ms/step - loss: 25.6566 - val_loss: 1.6543\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 1.1011 - val_loss: 0.9733\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.8101 - val_loss: 1.2899\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 1.1323 - val_loss: 0.8391\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.8537 - val_loss: 0.5552\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.7836 - val_loss: 0.7285\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6763 - val_loss: 1.1901\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 1.0350 - val_loss: 0.2397\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2926 - val_loss: 0.2799\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.3044 - val_loss: 0.2815\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3052 - val_loss: 0.3467\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.3394 - val_loss: 0.2916\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.3152 - val_loss: 0.3920\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4840 - val_loss: 5.6690\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 3.2251 - val_loss: 0.2610\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2268 - val_loss: 0.2277\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2206 - val_loss: 0.2262\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2161 - val_loss: 0.2270\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2171 - val_loss: 0.2225\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2150 - val_loss: 0.2215\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2140 - val_loss: 0.2215\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2124 - val_loss: 0.2267\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2149 - val_loss: 0.2186\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 11ms/step - loss: 0.2127 - val_loss: 0.2189\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2127 - val_loss: 0.2197\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 11ms/step - loss: 0.2132 - val_loss: 0.2180\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2124 - val_loss: 0.2201\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2131 - val_loss: 0.2214\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2119 - val_loss: 0.2199\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2117 - val_loss: 0.2246\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2146 - val_loss: 0.2200\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2139 - val_loss: 0.2181\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2126 - val_loss: 0.2202\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2153\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2055 - val_loss: 0.2147\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2058 - val_loss: 0.2145\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2057 - val_loss: 0.2157\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2059 - val_loss: 0.2157\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2061 - val_loss: 0.2136\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2052 - val_loss: 0.2136\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2063 - val_loss: 0.2154\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2054 - val_loss: 0.2147\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2060 - val_loss: 0.2165\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2055 - val_loss: 0.2153\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2051 - val_loss: 0.2130\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2060 - val_loss: 0.2158\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2054 - val_loss: 0.2130\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2060 - val_loss: 0.2159\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2082 - val_loss: 0.2135\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2181\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2058 - val_loss: 0.2129\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2046 - val_loss: 0.2169\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2049 - val_loss: 0.2176\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2064 - val_loss: 0.2132\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2040 - val_loss: 0.2148\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2042 - val_loss: 0.2161\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2055 - val_loss: 0.2147\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2043 - val_loss: 0.2205\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2023 - val_loss: 0.2137\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2017 - val_loss: 0.2136\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2017 - val_loss: 0.2136\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2116\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2020 - val_loss: 0.2138\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2013 - val_loss: 0.2138\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2020 - val_loss: 0.2134\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2023 - val_loss: 0.2137\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2014 - val_loss: 0.2135\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2019 - val_loss: 0.2135\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2017 - val_loss: 0.2117\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2009 - val_loss: 0.2126\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2005 - val_loss: 0.2128\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2005 - val_loss: 0.2125\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2004 - val_loss: 0.2128\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2007 - val_loss: 0.2123\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2010 - val_loss: 0.2129\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2014 - val_loss: 0.2131\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2005 - val_loss: 0.2128\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2007 - val_loss: 0.2128\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2014 - val_loss: 0.2126\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2004 - val_loss: 0.2129\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2008 - val_loss: 0.2128\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2010 - val_loss: 0.2130\n",
      "Fold 2 NN: 0.2116\n",
      "CV 3/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 13ms/step - loss: 25.1836 - val_loss: 0.8318\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6045 - val_loss: 0.5699\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.5980 - val_loss: 0.3885\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6727 - val_loss: 0.4867\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.5336 - val_loss: 0.5026\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4848 - val_loss: 0.4726\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.5260 - val_loss: 0.4488\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4300 - val_loss: 0.2252\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2468 - val_loss: 0.6261\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 1.8201 - val_loss: 0.2895\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.3098 - val_loss: 0.2348\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2461 - val_loss: 0.2219\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2338 - val_loss: 0.2568\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2368 - val_loss: 0.3503\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.7626 - val_loss: 0.3276\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2574 - val_loss: 0.2317\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2356 - val_loss: 0.2164\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2248 - val_loss: 0.2413\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2272 - val_loss: 0.2952\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2360 - val_loss: 0.2352\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2350 - val_loss: 0.2624\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2335 - val_loss: 0.2246\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2406 - val_loss: 0.2680\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2345 - val_loss: 0.2121\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2274 - val_loss: 0.2292\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2444 - val_loss: 0.2420\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2369 - val_loss: 0.2936\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2360 - val_loss: 0.2229\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.3068 - val_loss: 0.2209\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2415 - val_loss: 0.2167\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2225 - val_loss: 0.2460\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2118 - val_loss: 0.2112\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2066 - val_loss: 0.2112\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2103\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2057 - val_loss: 0.2119\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2052 - val_loss: 0.2094\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2046 - val_loss: 0.2094\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2042 - val_loss: 0.2098\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2059 - val_loss: 0.2098\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2048 - val_loss: 0.2113\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2041 - val_loss: 0.2105\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2057 - val_loss: 0.2100\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2059 - val_loss: 0.2126\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2021 - val_loss: 0.2091\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2019 - val_loss: 0.2093\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2017 - val_loss: 0.2098\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2019 - val_loss: 0.2095\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2017 - val_loss: 0.2093\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2017 - val_loss: 0.2094\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2018 - val_loss: 0.2093\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2014 - val_loss: 0.2098\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2010 - val_loss: 0.2091\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2019 - val_loss: 0.2093\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2005 - val_loss: 0.2093\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2003 - val_loss: 0.2091\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2006 - val_loss: 0.2092\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2011 - val_loss: 0.2095\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2009 - val_loss: 0.2095\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2007 - val_loss: 0.2092\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2008 - val_loss: 0.2092\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2007 - val_loss: 0.2091\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2012 - val_loss: 0.2092\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2009 - val_loss: 0.2091\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1999 - val_loss: 0.2092\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2007 - val_loss: 0.2091\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2005 - val_loss: 0.2092\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2011 - val_loss: 0.2092\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2005 - val_loss: 0.2092\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2011 - val_loss: 0.2092\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2005 - val_loss: 0.2092\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2093\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2007 - val_loss: 0.2092\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2008 - val_loss: 0.2092\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2005 - val_loss: 0.2092\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.1994 - val_loss: 0.2092\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2006 - val_loss: 0.2092\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2003 - val_loss: 0.2092\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2004 - val_loss: 0.2092\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2001 - val_loss: 0.2092\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1997 - val_loss: 0.2092\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2006 - val_loss: 0.2092\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2003 - val_loss: 0.2092\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2007 - val_loss: 0.2092\n",
      "Fold 3 NN: 0.20909\n",
      "CV 4/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 14ms/step - loss: 21.0162 - val_loss: 1.3988\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.8092 - val_loss: 0.5916\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6937 - val_loss: 0.4825\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.7655 - val_loss: 0.8508\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.8862 - val_loss: 0.5065\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.6349 - val_loss: 1.6054\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 1.0557 - val_loss: 0.2387\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2608 - val_loss: 0.2433\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2572 - val_loss: 0.2786\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2551 - val_loss: 0.2766\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2777 - val_loss: 0.2326\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2827 - val_loss: 0.2890\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 1.4429 - val_loss: 0.2607\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2625 - val_loss: 0.2354\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2347 - val_loss: 0.3021\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2314 - val_loss: 0.2304\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2401 - val_loss: 0.2480\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2352 - val_loss: 0.2200\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2412 - val_loss: 0.2533\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 11ms/step - loss: 0.2361 - val_loss: 0.3906\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2492 - val_loss: 0.2280\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2424 - val_loss: 0.2520\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2354 - val_loss: 0.2253\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2500 - val_loss: 0.2363\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2405 - val_loss: 0.2754\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2151 - val_loss: 0.2162\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2059 - val_loss: 0.2175\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2060 - val_loss: 0.2172\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2067 - val_loss: 0.2171\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2066 - val_loss: 0.2183\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2068 - val_loss: 0.2150\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2055 - val_loss: 0.2173\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2054 - val_loss: 0.2202\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2066 - val_loss: 0.2210\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2051 - val_loss: 0.2204\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2068 - val_loss: 0.2182\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2102 - val_loss: 0.2169\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2082 - val_loss: 0.2170\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2032 - val_loss: 0.2154\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2034 - val_loss: 0.2153\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2030 - val_loss: 0.2152\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2025 - val_loss: 0.2169\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2028 - val_loss: 0.2158\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2024 - val_loss: 0.2170\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2021 - val_loss: 0.2160\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2023 - val_loss: 0.2158\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2015 - val_loss: 0.2159\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2016 - val_loss: 0.2160\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2016 - val_loss: 0.2156\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2018 - val_loss: 0.2159\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2011 - val_loss: 0.2156\n",
      "Fold 4 NN: 0.21499\n",
      "CV 5/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 14ms/step - loss: 32.7954 - val_loss: 0.9868\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 1.0166 - val_loss: 0.6044\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.5945 - val_loss: 0.4791\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5925 - val_loss: 0.6527\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.5912 - val_loss: 0.7418\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4774 - val_loss: 0.2477\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2717 - val_loss: 0.2551\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2943 - val_loss: 0.2622\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.3032 - val_loss: 0.2543\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.3138 - val_loss: 0.3060\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.3080 - val_loss: 1.2261\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 5.7817 - val_loss: 0.3898\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4181 - val_loss: 0.2534\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2281 - val_loss: 0.2269\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2222 - val_loss: 0.2264\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2210 - val_loss: 0.2256\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2203 - val_loss: 0.2251\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2194 - val_loss: 0.2236\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2172 - val_loss: 0.2251\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2175 - val_loss: 0.2251\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2163 - val_loss: 0.2309\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2175 - val_loss: 0.2212\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2160 - val_loss: 0.2225\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2160 - val_loss: 0.2213\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2175 - val_loss: 0.2268\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2172 - val_loss: 0.2233\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2181 - val_loss: 0.2310\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2173 - val_loss: 0.2201\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2155 - val_loss: 0.2221\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2146 - val_loss: 0.2227\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2127 - val_loss: 0.2233\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2151 - val_loss: 0.2193\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2128 - val_loss: 0.2395\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2213 - val_loss: 0.2242\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2242 - val_loss: 0.2180\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2119 - val_loss: 0.2575\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2216 - val_loss: 0.2241\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2121 - val_loss: 0.2301\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2131 - val_loss: 0.2410\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2158 - val_loss: 0.2295\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2167 - val_loss: 0.2746\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2155 - val_loss: 0.2186\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2051 - val_loss: 0.2162\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2046 - val_loss: 0.2158\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2045 - val_loss: 0.2156\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2047 - val_loss: 0.2168\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2049 - val_loss: 0.2157\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2045 - val_loss: 0.2228\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2052 - val_loss: 0.2200\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2044 - val_loss: 0.2170\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2043 - val_loss: 0.2148\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2041 - val_loss: 0.2161\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2049 - val_loss: 0.2158\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2039 - val_loss: 0.2182\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2061 - val_loss: 0.2169\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2035 - val_loss: 0.2189\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2151\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2048 - val_loss: 0.2160\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2016 - val_loss: 0.2157\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2017 - val_loss: 0.2160\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2018 - val_loss: 0.2154\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2023 - val_loss: 0.2149\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2015 - val_loss: 0.2159\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2015 - val_loss: 0.2149\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2018 - val_loss: 0.2148\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2015 - val_loss: 0.2150\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2016 - val_loss: 0.2148\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2018 - val_loss: 0.2147\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2019 - val_loss: 0.2151\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2012 - val_loss: 0.2147\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2015 - val_loss: 0.2149\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2011 - val_loss: 0.2148\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2026 - val_loss: 0.2149\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2016 - val_loss: 0.2148\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2018 - val_loss: 0.2149\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2016 - val_loss: 0.2149\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2017 - val_loss: 0.2148\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2010 - val_loss: 0.2149\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2010 - val_loss: 0.2149\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2015 - val_loss: 0.2148\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2009 - val_loss: 0.2148\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2013 - val_loss: 0.2148\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2024 - val_loss: 0.2149\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2015 - val_loss: 0.2149\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2014 - val_loss: 0.2149\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2010 - val_loss: 0.2148\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2013 - val_loss: 0.2149\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2009 - val_loss: 0.2148\n",
      "Fold 5 NN: 0.21466\n"
     ]
    }
   ],
   "source": [
    "target_name='target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train_nn)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "train_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "test_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "\n",
    "train_nn[pred_name] = 0\n",
    "test_nn[target_name] = 0\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
    "\n",
    "for n_count in range(n_folds):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
    "    \n",
    "    X_train = train_nn.loc[train_nn.time_id.isin(indexes), features_to_consider]\n",
    "    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n",
    "    X_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), features_to_consider]\n",
    "    y_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.006), #0.006\n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_data = X_train[features_to_consider]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
    "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n",
    "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    counter += 1\n",
    "    features_to_consider.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4182e374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T01:36:14.115110Z",
     "iopub.status.busy": "2021-09-07T01:36:14.114459Z",
     "iopub.status.idle": "2021-09-07T01:36:14.135342Z",
     "shell.execute_reply": "2021-09-07T01:36:14.135777Z",
     "shell.execute_reply.started": "2021-08-26T05:24:19.99965Z"
    },
    "papermill": {
     "duration": 4.228799,
     "end_time": "2021-09-07T01:36:14.135941",
     "exception": false,
     "start_time": "2021-09-07T01:36:09.907142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSPE NN: 1.0 - Folds: [0.20807, 0.2116, 0.20909, 0.21499, 0.21466]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.001655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>0.002188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>0.002188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.001655\n",
       "1   0-32  0.002188\n",
       "2   0-34  0.002188"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_nn[\"row_id\"] = test_nn[\"stock_id\"].astype(str) + \"-\" + test_nn[\"time_id\"].astype(str) \n",
    "test_nn[target_name] = (test_predictions_nn*0.72+predictions_lgb_1*0.28) #(test_predictions_nn*0.585+predictions_lgb_1*0.415)\n",
    "# test_nn[target_name] = (test_predictions_nn*0.5+predictions_lgb_1*0.285+test_predictions_TabNet)\n",
    "score = round(rmspe(y_true = train_nn[target_name].values, y_pred = train_nn[pred_name].values),5)\n",
    "print('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n",
    "\n",
    "display(test_nn[['row_id', target_name]].head(3))\n",
    "test_nn[['row_id', target_name]].to_csv('submission.csv',index = False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4283.78205,
   "end_time": "2021-09-07T01:36:20.548543",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-07T00:24:56.766493",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
